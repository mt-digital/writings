@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/mt/Documents/MendeleyPDFs/323533a0.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
number = {6088},
pages = {533--536},
pmid = {134},
title = {{Learning representations by back-propagating errors}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learning+representations+by+back-propagating+errors{\&}ots=zZDj2mGYVQ{\&}sig=mcyEACaE{\_}ZB4FB4xsoTgXgcbE2g$\backslash$nhttp://books.google.com/books?hl=en{\&}lr={\&}id=FJblV{\_}iOPjIC{\&}oi=fnd{\&}pg=PA213{\&}dq=Learn},
volume = {323},
year = {1986}
}
@article{Ling1994,
abstract = {Learning the past tense of English verbs | a seemingly minor aspect of language acquisition | has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several arti cial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under di erent representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we o er insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.},
archivePrefix = {arXiv},
arxivId = {cs/9402101},
author = {Ling, Charles X.},
eprint = {cs/9402101},
file = {:Users/mt/Documents/MendeleyPDFs/9402101v1.pdf:pdf},
journal = {Journal of Artificial Intelligence},
pages = {209--229},
title = {{Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models}},
year = {1994}
}
@article{LeCun1998,
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/mt/Documents/MendeleyPDFs/lecun-98.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
pmid = {15823584},
title = {{Gradient-Based Learning Applied to Document Recognition}},
year = {1998}
}
@article{McClelland2003,
abstract = {How do we know what properties something has, and which of its properties should be generalized to other objects? How is the knowledge underlying these abilities acquired, and how is it affected by brain disorders? Our approach to these issues is based on the idea that cognitive processes arise from the interactions of neurons through synaptic connections. The knowledge in such interactive and distributed processing systems is stored in the strengths of the connections and is acquired gradually through experience. Degradation of semantic knowledge occurs through degradation of the patterns of neural activity that probe the knowledge stored in the connections. Simulation models based on these ideas capture semantic cognitive processes and their development and disintegration, encompassing domain-specific patterns of generalization in young children, and the restructuring of conceptual knowledge as a function of experience.},
author = {McClelland, James L and Rogers, Timothy T},
doi = {10.1038/nrn1076},
file = {:Users/mt/Documents/MendeleyPDFs/nrn1076.pdf:pdf},
isbn = {1471-003X},
issn = {1471-003X},
journal = {Nat. Rev. Neurosci.},
keywords = {Animals,Cerebral Cortex,Cerebral Cortex: anatomy {\&} histology,Cerebral Cortex: physiology,Cognition,Cognition: physiology,Humans,Learning,Learning Disorders,Learning Disorders: physiopathology,Learning: physiology,Models,Nerve Net,Nerve Net: anatomy {\&} histology,Nerve Net: physiology,Neural Pathways,Neural Pathways: anatomy {\&} histology,Neural Pathways: physiology,Neurological,Semantics},
number = {4},
pages = {310--322},
pmid = {12671647},
title = {{The parallel distributed processing approach to semantic cognition.}},
url = {http://dx.doi.org/10.1038/nrn1076},
volume = {4},
year = {2003}
}
@misc{Hof2013,
abstract = {With massive amounts of computational power, machines can now recognize objects and translate speech in real time. Artificial intelligence is finally getting smart.},
author = {Hof, Robert D.},
booktitle = {MIT Technology Review},
title = {{Deep Learning}},
url = {https://www.technologyreview.com/s/513696/deep-learning/},
urldate = {2016-09-18},
year = {2013}
}
@misc{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael A.},
publisher = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2015}
}
@article{Joanisse2015,
abstract = {The field of formal linguistics was founded on the premise that language is mentally represented as a deterministic symbolic grammar. While this approach has captured many important characteristics of the world's languages, it has also led to a tendency to focus theoretical questions on the correct formalization of grammatical rules while also de-emphasizing the role of learning and statistics in language development and processing. In this review we present a different approach to   language research that has emerged from the parallel distributed processing or 'connectionist' enterprise. In  the connectionist framework, mental operations are studied by simulating learning and processing within networks of artificial neurons. With that in mind, we discuss recent progress in connectionist models of auditory word recognition, reading, morphology, and syntactic processing. We argue that connectionist models can capture many important characteristics of how language is learned, represented, and processed, as well as providing new insights about the source of these behavioral patterns. Just as importantly, the networks naturally capture irregular (non-rule-like) patterns that are common within languages, something that has been difficult to reconcile with rule-based accounts of language without positing separate mechanisms for rules and exceptions. For further resources related to this article, please visit the $\backslash$nWIREs website. Conflict of interest: The authors have declared no conflicts of interest for this article.},
author = {Joanisse, Marc F. and McClelland, James L.},
doi = {10.1002/wcs.1340},
file = {:Users/mt/Library/Application Support/Mendeley Desktop/Downloaded/Joanisse, McClelland - 2015 - Connectionist perspectives on language learning, representation and processing.pdf:pdf},
issn = {19395086},
journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
number = {3},
pages = {235--247},
pmid = {26263227},
title = {{Connectionist perspectives on language learning, representation and processing}},
volume = {6},
year = {2015}
}
@misc{Tensorflow.org,
author = {Tensorflow.org},
title = {{MNIST For ML Beginners}},
url = {https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html},
urldate = {2016-09-18}
}
@misc{Tensorflow.orga,
abstract = {TensorFlow™ is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.},
author = {Tensorflow.org},
title = {{TensorFlow — an Open Source Software Library for Machine Intelligence}},
url = {https://www.tensorflow.org/},
urldate = {2016-09-18}
}
@misc{Inceptionism,
title = {{Research Blog: Inceptionism: Going Deeper into Neural Networks}},
url = {https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}
}
@book{Rumelhart1986a,
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Rumelhart, David E. and McClelland, James L.},
booktitle = {Parallel Distributed Processing},
doi = {10.1016/0010-0277(93)90006-H},
eprint = {9809069v1},
file = {:Users/mt/Documents/MendeleyPDFs/Chapter18.pdf:pdf},
isbn = {0262521873},
issn = {00100277},
keywords = {connectionism},
pmid = {9409667},
primaryClass = {arXiv:gr-qc},
publisher = {Massachusetts Institute of Technology},
title = {{Parallel Distributed Processing}},
url = {http://stanford.edu/{~}jlmcc/papers/PDP/Chapter18.pdf},
year = {1986}
}
